[{"content":"Harnessing KNN \u0026amp; Neural Nets to Assess Default Risk for Unbanked Loan Applicants\nIntroduction Lending money is a profitable enterprise but only if you’re loaning to people that pay back their loans. Therefore, it is very important for banks and companies to be able to assess beforehand whether or not a person is likely to repay a loan. Many criteria have been considered over the years in attempts to gauge the responsibility of a borrower including but not limited to the borrower’s previous loan history, the area in which they are living, and how long it has been since they changed phone numbers. One of the key metrics used in the credit history, a measure of how you have behaved with previous loans calculated from how much you borrow, how often you make late payments, and how often you default. Most lending entities today prioritize credit history as their metric of choice when considering loan applications and often will not loan to applicants who cannot present a satisfactory credit history. However, this leads to a problem for both the loaners and the borrowers: the borrowers cannot get the capital they might need without submitting to terrible loan deals and the loaners leave a large population of customers untapped. By analyzing nonstandard metrics other than credit history to predict the trustworthiness of a borrower, this analysis seeks to bridge this gap. An accurate classification model could provide the loan entities with some assurance that their loans will be repaid and that, with less risk, they would be more willing to loan to this non borrowing population, benefiting everyone.\nTo address this issue, an initial exploratory factor analysis (EFA) is conducted, in which I attempt to recover latent variables relating to clients’ trustworthiness. The hope is that such observations will be informative to future attempts at classification and in the compiling of other data sets. In other words, by finding which variables in our data set are most informative, companies and researchers can prioritize them both at the data collection stage and at the evaluation stage.\nAfter this, a k-nearest neighbors classifier is applied to the data to form a predictive model. This model, if accurate, would be a major deliverable of the project and ready for use by loan entities. At the very least this attempt at classification will inform us as to whether or not the quantitative variables being collected and used are informative to the question of interest.\nA logistic neural net is also applied (with the same rationale as the k-nearest neighbor model). It’s chosen because we are interested in seeing whether we can do a better job in predicting whether or not a client will default on a loan. A neural net is a platform for many machine learning techniques. A system of neuron nodes are set up each with a different random weights and connected to a random selection of the input variables. The process runs on the training set to classify the data and then adjusts the weights of its neurons based on the most erroneous cases, repeating until the weights of the neurons are no longer adjusting or until some other specified cutoff point is reached. By using both of these classification methods, we will compare the two models and either confirm the accuracy or our predictions or indicate more work is needed on one model or the other.\nData The data used comes from the Home Credit Group which is interested in giving loans to clients that have no credit history. As such, other metrics must be used and the challenge of this dataset is evaluating a borrower’s likelihood of repaying from these other criteria. The 307511 observation dataset of loans contains 122 alternative variables and is not terribly tidy. Some extra work is needed before working with the dataset. Firstly, a uniform random sample of the dataset is taken to cut it down to 10,000 observations. We then pare down the dataset’s variables by eliminating any with less than 90% complete observations. This leaves us with 66 variables, most of them indicator flags of whether an applicant met this or that criteria. Upon some further trimming, we decide to use the following nine quantitative variables:\n  Target: binary indicator of default, 0 is no default, 1 is default.\n  Income: annual income of the borrower\n  Loan amount: the dollar amount of the requested loan\n  Loan annuity: how much collateral the borrower could present\n  Region Population: the normalized level of population for that area\n  Client Age: age in years\n  Length of Employment: negative values indicate unemployment\n  Number of Family Members\n  Days Since Last Phone Change\n  Extraction We query an SQLite database to obtain client information using the DBI package. The dbConnect() function is used to create a database connection. Invoking it returns an object that can be used to communicate with the database management system (DBMS), providing access to dynamic queries, results and management sessions.\n ## Target Income Amount Annuity Pop Age Emp Fam Phone ## 1 0 157500 288873 14805.0 0.022625 -10676 -1190 2 -2 ## 2 0 207000 781920 34573.5 0.046220 -21762 365243 1 -882 ## 3 0 103500 161730 11385.0 0.024610 -17623 -1636 3 -2293 ## 4 0 405000 1800000 47614.5 0.010006 -13362 -232 4 -309 ## 5 0 157500 545040 25537.5 0.025164 -13115 -6211 1 0 ## 6 0 157500 314055 17167.5 0.003122 -16330 -1971 3 -808  The table above shows the first six client entries after obtaining the 9 variables of interest.\nExploratory Analysis A correlation matrix [Link] is constructed from the quantitative variables to get a closer look at correlation values as well as the distributions of the variables in our dataset. Most of the correlations were weak. The only strong correlation observed was between loan annuity and loan amount which makes sense since we would expect the bank to be willing to loan more money if more collateral is presented. Most of the variables are right skewed. It is also interesting to see the presence of multiple distinct peaks among the different variable distributions.\ninsert correlation matrix image and caption\nMethods Experiments In addition to being listed on this report, the experiments conducted in this study are shared and maintained on Tableau Public. Feel free to check out my published worksheet below!\ntableau link here\nFactor Analysis A Factor Analysis is more appropriate in this analysis than Principal Components Analysis because we are interested in seeing which latent indicators of trustworthiness / ability to pay back loans can be recovered from a set of manifest variables. A Maximum Likelihood Approach is used to determine, through repeated hypothesis testing, which number of factors is most appropriate. Add map2\nClassification  K Nearest Neighbors:\n  KNN classifiers with 10 values of k (1-10) are fitted and represented on plots showing the apparent and estimated error rates for the various values of k. An 80% ~ 20% holdout sample approach is used here to obtain the respective train and test sets.   Neural Nets:\n  The second classification method used was neural nets. Neural nets are composed of layers of weighted neurons that pass on 0 or 1 depending on whether the weighted sums of their inputs exceed their activation potential. The system is modeled after the function of the neurons in the human brain. By working on the training set iteratively, the weights of the neurons can be refined based on the misclassifications until all training samples are classified (or, to reduce over fitting, until a certain number of iterations have been reached). By layering these sets of neurons and having the outputs of one layer be fed into the next, you can do this weight-refining approach several times and achieve accuracy in complex classifications far beyond the scope of regression, as evidenced by their use in photo analysis, voice identification, and of course in our own heads.  In this case, a holdout sample approach was used to evaluate the sufficiency of the neural network model. The proportions chosen were 75% ~ 25% for the train and test set respectively. The caret package is used to find the best parameters to use for this classifier. To kick start the process, we set up a grid of tuning parameters for the model, fitted each and calculated a bootstrapped AUC (Area under ROC curve) score for every hyper parameter combination. The values with the biggest AUC are chosen for the neural network. The preferred evaluation metric is AER and estimated TER, but the only available ones are: sensitivity, specificity, area under the ROC and AUC. It is worth noting that all available predictors are used to fit the model. The final parameters chosen are: three interconnected neuron layers with a decay rate of 0.5 (factor by which each weight is multiplied by after each update.)1 [link]. The weighted inputs and bias are summed to form the net input to the next layer. The inputs are mapped from layer to layer then finally fed into a sigmoid function that outputs the desired probability that a client will default.\nResults Factor Analysis Five possible factor solutions are examined to find one with the most compelling interpretation. Although the p-values yielded from the maximum likelihood evaluation of the five factor solution suggested that none of them could adequately account for the variations in the data, the factors in the five factor model satisfy the principles of Thurstone’s simple structure. The first factor in this model loads highly on credit and annuity thus it could be labelled non-Income assets. The second one is a contrast between age and duration of employment. The third is dominated by continuous family members therefore it could be seen as a measure of client mobility. The fourth reflects income whereas the fifth seems like a weighted average.\nInsert table here\nK Nearest Neighbors Figure1 is the graph of the AER and TER for the KNN runs on values of K from 1:10. Larger values of k perform fairly well. Additionally, we see a convergence toward a 0.08 error rate in both the AER and the TER. However, due to the nature of our data, we shouldn’t treat all error rates equally. From the viewpoint of the company, mistakenly classifying a paying borrower as nonpaying is less harmful than classifying a nonpaying borrower as paying. In other words, a false positive from our classifier means we give a loan that is not paid back. Let’s consider the error rate of only these harmful errors in Figure2.\nWe can see in Figure2 that the AER behaves essentially identically to the run with both error rates while the TER is markedly different in the beginning but then both continue to approach 0.08. This suggests that, as we increase k, we eliminate the Type I errors of our model on new data. Unfortunately the harmful Type II error rate is not eliminated and remains at 0.08. This means that we should expect 8% of the applicants given the go-ahead by our model to actually not repay their loans. On the upside however, if our model indicates an applicant cannot be trusted, it is almost always correct.\nGraph 1\nGraph 2\nNeural Networks The results obtained from the grid search process show that the prediction accuracy increases as a function of the number of units in the hidden layer of the neural network. There is also a discernible improvement in model quality for moderate weight decay values. The estimated true error rate was 8.64% and the apparent error rate was 8.87%. Both of these error rates are based on a majority class prediction.\nInsert graph here\nConclusions From the results obtained from this analysis, we can reasonably recommend the adaptation of a KNN classifier due to its robust performance in predicting default risk. Although a neural network performs similarly, it proved to be an extremely slow learner that takes long to run. More data and resamples would have been brought in given more time and resources to see whether the observed issues with multivariate normality can be fixed. It is also important to note that the scope of inference from the methods used in this analysis are only applicable to individuals with similar histories to those in this study.\nCitations  Brownlee, Jason. “Tuning Machine Learning Models Using the Caret R Package.” Machine Learning Mastery, 22 Sept. 2016, www.machinelearningmastery.com/tuning-machine-learning-models-using-the-caret-r-package/. Portilla, Jose. “KDnuggets.” KDnuggets Analytics Big Data Data Mining and Data Science, 2016, www.kdnuggets.com/2016/08/begineers-guide-neural-networks-r.html. “Weight Decay in Neural Networks.” Metacademy, metacademy.org/graphs/concepts/weight_decay_neural_networks. R documentation caret package Multivariate Analysis With R Wickham, H. (n.d.). R Database Interface (DBI 1.1.0). Retrieved December 11, 2020, from https://dbi.r-dbi.org/   .emojify { font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols; font-size: 2rem; vertical-align: middle; } @media screen and (max-width:650px) { .nowrap { display: block; margin: 25px 0; } } ","permalink":"https://adityatelange.github.io/hugo-PaperMod/posts/defaultriskcalculations/","summary":"\u003cp\u003eHarnessing KNN \u0026amp; Neural Nets to Assess Default Risk for Unbanked Loan Applicants\u003c/p\u003e","title":"Navigating the Unknown"},{"content":"Exposition of forecasting methods for modelling changing variance in time series\nAbstract This project is based on exploring ARCH /GARCH methods and their application in modelling the changing variance of time in predicting stock prices. The data used in this study is obtained from the S\u0026amp;P 500 index, which is a measure that estimates the performance of 500 companies listed in the United States Stock exchange market. The data used includes daily data spanning the years 2013 - 2018.\nIntroduction - ARIMA models The first phase of this analysis begins with the exploration of an ARIMA, where ARIMA stands for Auto Regressive Moving Average models. They consist of two components; the Autoregressive Component and the Moving Average Component and are denoted as ARIMA(p,d,q), with p representing the number of autoregressive terms, d the number of differencing and q the number of Moving Average terms. The second phase involves the checking of model residuals (to look for volatile clusters) followed by an eventual transition to ARCH/GARCH.\nBefore we begin any model fitting, we make the line graphs shown below. This step is meant as an initial exploration aimed at showing the trends in volume and price of Chesapeake Energy Corporation’s stocks. All the plots below are similar because they show that the volume, opening, high and low values exhibited high volatility. Volatility in a time series refers to the phenomenon where the conditional variance of a time series varies over time (Cryer and Chan, 2008). Stock volume seems more volatile than low, high and close prices, as can be seen from the more sudden irregular shifts in trends with time.\nThe data wrangling required before fitting the model/ checking conditions is minimal. It begins by decomposing Chesapeake monthly data time series into seasonal, trend and irregular components, then moves on to the removal of seasonal components to create a seasonally adjusted component.\nInsert image here\nModel Conditons Before fitting an ARIMA time series model, we need to make sure that it is free of trends and non seasonal behavior. We also check to make sure that the time series has a constant mean and variance. If variation in trends is present, we difference in order to get rid of those trends and prepare the data for model fitting. After all these checks are performed, we run the Augmented Dickey Fuller test to make sure that stationarity is satisfied:\nThe hypothesis test to check whether our data is stationary is as follows:\n $H_0$: Chesapeake time series is not stationary.  $H_A$: Chesapeake time series is stationary. Our test yields a statistic of −3.5423 and a p value of 0.03823. We therefore reject the null as there is strong evidence of stationarity in the data, and proceed to the model fitting phase.\n#adf test checks whether ts is stationary or not (data condition) adf.test(CHK_deseasonal_value, alternative = \"stationary\")  ## ## Augmented Dickey-Fuller Test ## ## data: CHK_deseasonal_value ## Dickey-Fuller = -3.5423, Lag order = 10, p-value = 0.03823 ## alternative hypothesis: stationary  Model Fitting Here, we use the Auto.arima() function to help obtain model parameters using a stepwise model fitting procedure. This model selection procedure selects the model with the lowest AIC value. The p, d, q parameters of the model will be selected from the model with the lowest score. We start with a maximum order of 6 for all parameters and iterate through different combinations to find one that produces the model with the lowest AIC score:\n#auto fits arima model CHK_fit = auto.arima(CHK_deseasonal_value, max.order = 6) CHK_fit  ## Series: CHK_deseasonal_value ## ARIMA(4,1,3) ## ## Coefficients: ## ar1 ar2 ar3 ar4 ma1 ma2 ma3 ## 0.2097 0.3550 0.6572 -0.4885 0.2163 -0.3094 -0.7638 ## s.e. 0.0449 0.0418 0.0359 0.0263 0.0468 0.0452 0.0373 ## ## sigma^2 estimated as 1.864e+12: log likelihood=-19460.3 ## AIC=38936.59 AICc=38936.71 BIC=38977.65  The model chosen from our selection procedure has 4 Autoregressive Terms i.e AR(4), a differencing of degree 1 and 3 moving average terms i.e MA(3). The fitted model from the parameters obtained above can be expressed as :\n $$\\hat{Y}_{t} = 0.2097 Y_{t-1}+0.3550 Y_{t-2}+0.6572 Y_{t-3}-0.4885 Y_{t-4}+0.2163 e_{t-1}-0.3094 e_{t-2}-0.7638 e_{t-3}+\\epsilon$$ The equation above is a linear combination of terms. The Y’s correspond to recent stock volume values up until the $(t-4)^{th}$ time step while the $e$\u0026rsquo;s correspond to the errors of the lags at the denoted, corresponding time steps.\nIn the next section we shall examine sample partial autocorrelation (PACF) and sample autocorrelation plots (ACF) to validate our choices of the p, d and q orders chosen for our ARIMA model by the stepwise model selection procedure.\nModel Diagnostics Before settling down on the model obtained from the previous section, we examine the auto correlation of residuals to make sure our model is free of auto correlations. This is necessary because it helps us establish whether the noise terms in the model are independent from each other:\nggtsdisplay(residuals(CHK_fit), plot.type = \"partial\", main = \"ARIMA (4, 1, 3) Diagnostic plots\")  Insert diagnostic plots here\nA quick examination of the ACF plot reveals the existence of excessive correlations in the residuals at lags 3 and 6. Furthermore, the lag patterns in the PACF are quite similar to those in the ACF plot, suggesting the existence of autocorrelation. This problem in distribution of error terms also manifests itself in the residuals, and can be seen from the unequal variation of error terms across the range of values in the residual plot.\nIn order to confirm the findings from the diagnostic plots above, we conduct an official hypothesis test for model fit using the Ljung - Box test to see whether the error terms in the model are independently and identically distributed (i.i.d).\nThe Ljung Box test statistic is given by (Glen, 2018): $$Q_* = n(n+2) \\sum_{k=1}^{m} \\frac{r_{k}^{2}}{n-k}$$ where n is the size of the time series and r the residual correlation at the $k^{th}$ lag . $Q_*$ has a chi-square distribution with k-p-q degrees of freedom (Cryer and Chan, 2008). The official hypothesis for the test are as follows:\n$H_0$ : The model error terms are uncorrelated\n$H_A$ : The model error terms are correlated\nBox.test(residuals(CHK_fit), lag = 90, fitdf = 83, type = 'Ljung-Box')  ## ## Box-Ljung test ## ## data: residuals(CHK_fit) ## X-squared = 541, df = 7, p-value  Running the test yields a p value of 2.2e-16. We have sufficient evidence to reject the null, as there is strong evidence that the model assumption of independence of error terms has been violated. In the next section we attempt to find the remedy to this problem by exploring methods that model the changing variance in time series.\nARCH/ GARCH models Introduction In the previous chapter we tried fitting and assessing the feasibility of an ARIMA model on Chesapeake Energy Corporation’s stock data. The fitted model wasn’t appropriate because the model’s error terms were not independent and identically distributed. In addition, cluster volatility seemed to be a huge issue as observed from the heteroschedastic nature of modeled stock volume returns.\nIn this section we will use autoregressive conditional heteroschedastic models in an attempt to adequately capture and account for the heteroscedasticity observed in the ARIMA model. ARCH/GARCH are time series models used to model processes where volatility is high in provided data (Cryer and Chan, 2008). Cases involving stock market data are usually prone to unpredictable changes, and are best modeled using methods that model the variability of future values based on present and past provided trends in observed returns.\n1. ARCH models ARCH models are denoted ARCH(p), where p represents the order of the model. According to Cryer and Chan (2008) an ARCH(1) process modelling the return of a time series r takes the form $r_{t}=\\sigma_{t | t-1} \\varepsilon_{t}$ where $\\varepsilon_{t}$ is a series of independent and identically distributed random variables with a mean of zero and standard deviation of 1. The quantity $\\sigma_{t | t-1}$ models the conditional variance, $\\sigma_{t | t-1}^{2}$, of the return $r_{t}$, and is given by $\\sigma_{t | t-1}^{2}=\\omega+\\alpha r_{t-1}^{2}$. The variance of the current return is based on conditioning upon returns until the ${(t-1)}^{th}$ time step. The quantities $\\omega$ and $\\alpha$ represent the ARCH model intercept and coefficient respectively.\nThe diagram below represents a sample ARCH(1) process simulated with $\\omega$ = 0.1 and $\\alpha$ = 0.5 as the chosen model parameters.\nset.seed(85) garch01.sim = garch.sim(alpha=c(.1,.5),n=400) sim_data  Insert Simulated arch process image\n2. GARCH models The ARCH model introduced in the previous section models future returns by conditioning the value of the variance at time t to the previous time step alone i.e $\\sigma_{t | t-1}^{2}=\\omega+\\alpha r_{t-1}^{2}$. Bollerslev\u0026rsquo;s (1986) approach encourages the backward extension of this conditioning process up until the $q^{th}$ time step as well as the introduction of p lags to the conditional variance (Cryer and Chan, 2008). This resulting model becomes a Generalized Autoregressive Conditional Heteroscedasticity (GARCH) process, and is denoted as GARCH(p,q). The return from this new proposed model takes the same form as ARCH\u0026rsquo;s $r_{t}=\\sigma_{t | t-1} \\varepsilon_{t}$. However, the conditional variance $\\sigma^2_{t | t-1}$ modeled by the quantity $\\sigma_{t | t-1}$ now becomes : $$\\begin{aligned} \\sigma_{t | t-1}^{2}=\\omega+\\beta_{1} \\sigma_{t-1 | t-2}^{2}+\\cdots+\u0026amp; \\beta_{p} \\sigma_{t-p | t-p-1}^{2}+\\alpha_{1} r_{t-1}^{2}+\\alpha_{2} r_{t-2}^{2}+\\cdots+\\alpha_{q} r_{t-q}^{2} \\end{aligned}$$\nThe $\\beta$ coefficients in the model are used to assign weights to the lags of the conditioned variance values.\nThe plot below (Cryer and Chan, 2008) illustrates an example of a simulated GARCH(1,1) process with parameter values $$\\omega=0.02, \\alpha=0.05, \\text { and } \\beta = 0.9$$\nInsert simulated garch process\nThe parameters $\\omega, \\beta, \\alpha$ in GARCH and $\\omega, \\alpha$ in ARCH are constrained to $\u0026gt;0$, since the conditional variances have to be positive.\nEstimation of GARCH model coefficents GARCH model coefficients are fit using the Maximum Likelihood Approach. The estimation process used to obtain the likelihood estimates for $\\omega, \\beta, \\alpha$ is based on recursively iterating through the log likelihood function modelling the GARCH coefficient estimates. The log likelihood function we aim to maximize is defined as (Cryer and Chan, 2008):\n$$L(\\omega, \\alpha, \\beta)=-\\frac{n}{2} \\log (2 \\pi)-\\frac{1}{2} \\sum_{i=1}^{n}\\left\\{\\log \\left(\\sigma_{t-1 | t-2}^{2}\\right)+r_{t}^{2} / \\sigma_{t | t-1}^{2}\\right\\}$$ Model Fitting Below, we try fitting an appropriate GARCH model using the ugarchfit() function from the rugarch package. The package computes the model estimates using the maximum likelihood function specified in the previous section.\nWe will explore different GARCH orders, with the aim of finding the model that best fits the data. The orders we will try are arbitrarily chosen as GARCH(1,1), GARCH(2,2)and GARCH(3,3):\nModel Diagnostics The GARCH(1,1) model seems like the most appropriate here since all but one of it\u0026rsquo;s model coefficients are significant. Furthermore, it has the lowest AIC of all the models explored.\nBefore we accept the model we found in the previous section we need to make sure that assumptions have been met for the ARCH(1,1) model. The squared residuals need to be serially uncorrelated and the error terms should be normally distributed:\n#QQplot to check for normality qqnorm(residuals(fit_mod1)); qqline(residuals(fit_mod1)) Insert QQ plot here\n#Sample acf and pacf plots to check iid assumption ggtsdisplay(residuals(fit_mod1), plot.type = \"partial\", main = \"Chesapeak Energy Corporation's GARCH (1, 1) Diagnostic plots\") Insert Another diagnostic plot\nFrom the diagnostic plots made above, we see a major problem with normality, as most of the points on the qqplot veer off the line. In addition, there seems to be major issues with independence, as can be seen from the significant lags in residuals from the autocorrelation plots.\nResults From the model diagnostics we ran in the previous section, we discovered major issues with normality and independence in error terms. We might want to explore more model parameters to see whether we can find a model that improves upon what we currently have. For now, we will proceed with extreme caution and use the GARCH(1,1) model to make some predictions.\nBelow, we extract the coefficients from the chosen GARCH(1,1) model.\n#Extract coefficients fit_mod1@fit$matcoef ## Estimate Std. Error t value Pr(|t|) ## mu 16.96828513 0.03046360 557.001998 0.000000e+00 ## omega 0.04592897 0.01059138 4.336448 1.448039e-05 ## alpha1 0.40407479 0.05760516 7.014559 2.306821e-12 ## beta1 0.51414297 0.06573577 7.821358 5.329071e-15 The return at time t as given by our model is going to be given by (Boudt, 2020) : $$R_{t} = 16.97 +e_{t}$$ where the $e_{t}$ is a normally distributed random variable with a mean of 0 and variance of $\\widehat{\\sigma}_{t}^{2}$ i.e $e_{t} \\sim N\\left(0, \\widehat{\\sigma}_{t}^{2}\\right)$. The variance modelling return volatility at the $t^{th}$ time step in our fitted model is going to be $\\widehat{\\sigma}_{t}^{2}=0.05+0.40 e_{t-1}^{2}+0.51 \\widehat{\\sigma}_{t-1}^{2}$\nThe ugarchroll() function is used to obtain estimates for the last four dates in the data set. The test data that is used is the most recent week\u0026rsquo;s returns in stock volume. The log volume residuals obtained after this process are printed down below :\n#Forecasting using the ugarchforecast function preds = ugarchforecast(fit_mod1, n.ahead = 1, data = CHK[1255:1259, ,drop = F][,6]) preds = ugarchroll(spec = spec_mod1 , data = log(CHK_ts) , n.start = 1255 , refit.every = 2 , refit.window = 'moving') References  Cryer, J. D., \u0026amp; Chan, K.-sik. (2008). Time series analysis with applications in R. New York: Springer. Nugent, C. (n.d.). S\u0026amp;P 500 stock data. Retrieved from https://www.kaggle.com/camnugent/sandp500. Boudt, K. (n.d.). GARCH Models in R. Retrieved from https://www.datacamp.com/courses/garch-models-in-r. Trapletti, A., \u0026amp; Hornik, K. (n.d.). Package ‘tseries.’ Retrieved from https://cran.r-project.org/web/packages/tseries/tseries.pdf Ghalanos, A., \u0026amp; Kley, T. (n.d.). Package ‘rugarch.’ Retrieved from https://cran.r-project.org/web/packages/rugarch/rugarch.pdf Glen, S. (2018, September 11). Ljung Box Test: Definition. Retrieved from https://www.statisticshowto.datasciencecentral.com/ljung-box-test/ Bollerslev, T. (1986). Generalized autoregressive conditional heteroskedasticity. Journal of Econometrics, 31(3), 307–327. doi: 10.1016/0304-4076(86)90063-1   .emojify { font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols; font-size: 2rem; vertical-align: middle; } @media screen and (max-width:650px) { .nowrap { display: block; margin: 25px 0; } } ","permalink":"https://adityatelange.github.io/hugo-PaperMod/posts/variancemodelling/","summary":"\u003cp\u003eExposition of forecasting methods for modelling changing variance in time series\u003c/p\u003e","title":"Modelling Changing Variance in Time Series"},{"content":"An online tool which sheds more light on the numbers behind the opioid crisis\nAbstract According to the U.S. Drug Enforcement Administration, “overdose deaths, particularly from prescription drugs and heroin, have reached epidemic levels”. From 1999 to 2016, more than 630,000 people have died from a drug overdose. On average, 115 Americans die every day from an opioid overdose.\\(^1\\)\nIntroduction Opioid abuse is a pervasive problem that has persisted and been responsible for the deaths of thousands and the decreasing life expectancy of Americans. While it is true that every strata of society is in danger from the effects of this epidemic, its effects vary widely across age groups and gender. This project is significant because it is an accessible way to know more about this silent disaster. The aim is that people that don’t know about the crisis will be able to talk knowledgeably about what it is and how it affects society, and that those who already know about it will be able to correct and enhance their understanding of the numbers behind the situation.\nThe first wave began with increased prescribing of opioids in the 1990s\\(^2\\), with overdose deaths involving prescription opioids (natural and semi-synthetic opioids and methadone) increasing since at least 1999\\(^3\\). The third wave began in 2013, with significant increases in overdose deaths involving synthetic opioids – particularly those involving illicitly-manufactured fentanyl (IMF). The IMF market continues to change, and IMF can be found in combination with heroin, counterfeit pills, and cocaine\\(^4\\).\nData The data we used for our analysis was obtained from the following sources:\nThe CDC. This is an online database belonging to the CDC. It contains wide ranging data for Epidemiologic Research. We tried using R packages to query the website’s API, but they were not as user friendly as we hoped. Eventually we resorted to performing direct queries on the website itself, but this had its own limitations in terms of size. Forty queries were done (each had a limit of 75,000 lines) to obtain 1999 - 2016 data on drug poisoning mortality rates.\n Data world website. This dataset from the The Centers for Medicare \u0026amp; Medicaid Services contains the information on the individual opioid prescribing rates of health providers that participate in Medicare Part D program. This file provides data on the number and percentage of prescription claims (includes new prescriptions and refills) for opioid drugs, and contains information on each provider’s name, specialty, state, and ZIP code. Available documentation can be found here.\n  Findings Patterns \u0026amp; Trends In the 2000s, the age groups that were most affected by drug related deaths were individuals that were 45 years or older. As the crisis expanded, and with the introduction of synthetic opioids such as Fentanyl in the early 2010s, the 25-34 age group became the most affected age group. Generally, all the age groups have experienced an upward trend, but this younger age group’s change is a good insight to keep in mind when we are finding ways to implement policies to address the drug crisis.\nThe trends across different racial groups show that the opioid crisis has predominantly affected the White population of the United States. However, the fact that the Black community’s deaths have more than sixtupled since 1999 cannot be overlooked. It is important to consider how the different races are affected by the problem. Additionaly, male mortality rate has stayed at a constant level above that of females with roughly twice as many deaths.\nThe problem is clearly a national problem, but it affects the young adult age group the most, and White people the most. Is the problem worse in certain regions of the country?\nVisualizations Below is a map that shows the top 10 counties that were most affected by the epidemic in 2016. The midwest seems to have been the most severely hit by the opioid epidemic.\nAdd map1\nThe choropleth map below is shaded in propotion to the number of age adjusted deaths in all states in 2014. Some of the cities with the highest Opioid prescription rates were Sacramento (CA), Piedmont (SD) and Stringtown (OK).\nAdd map2\nShiny Widget Add shiny\nhttps://raymondnzaba.shinyapps.io/OpioidCrisis/\nLimitations \u0026amp; Conclusion This research hones in on the casualties caused by drug poisoning and overdoses yet the opioid crisis is a multifaceted problem involving policing and a whole drug trafficking and distribution industry. More time and resources are needed to obtain data that would ideally enrich the perspectives on the problem presented herein. Information on drug prescriptions and arrest information would help break down this sensitive topic even more and show the true severity of the opioid epidemic. The app is posted on the internet and can be used by anyone as a tool to inform and educate.\nTechnical Appendix #Imports necessary packages library(tidyverse) library(ggplot2) library(readr) library(stringr) library(maps) library(ggmap) library(mapproj) library(leaflet) library(httr) library(plotly) library(leaflet) library(plotly) library(RColorBrewer) #Reads in the data that is grouped by Year, County, Drug Cause, and Age Groups for 1999-2016 YearCountyCauseAgeGroupsAllYears \u0026lt;- as_tibble() for (year in 1999:2016) { YearCountyCauseAgeGroupsAllYears \u0026lt;- rbind(YearCountyCauseAgeGroupsAllYears, read_csv(paste0(\u0026quot;../Data/WonderData/YearCountyCauseAgeGroups\u0026quot;,year,\u0026quot;.csv\u0026quot;))) } #Reads in the data that is grouped by Year, County, Drug Cause, and Gender for 1999-2016 YearCountyCauseGenderAllYears \u0026lt;- as_tibble() for (part in 1:9) { YearCountyCauseGenderAllYears \u0026lt;- rbind(YearCountyCauseGenderAllYears, read_csv(paste0(\u0026quot;../Data/WonderData/YearCountyCauseGender10States\u0026quot;,part,\u0026quot;.csv\u0026quot;))) } #Reads in the data that is grouped by Year, County, Drug Cause, and Race for 1999-2016 YearCountyCauseRaceHomicideAllYears \u0026lt;- as_tibble() for (part in 1:6) { YearCountyCauseRaceHomicideAllYears \u0026lt;- rbind(YearCountyCauseRaceHomicideAllYears, read_csv(paste0(\u0026quot;../Data/WonderData/YearCountyCauseRaceHomicideAllYears10States\u0026quot;,part,\u0026quot;.csv\u0026quot;))) } #Reads in the data that is grouped by Year, County, Drug Cause, and Race for 1999-2016 YearCountyCauseRaceAllYears \u0026lt;- rbind(read_csv(\u0026quot;../Data/WonderData/YearCountyCauseRaceUndeterminedAllYears.csv\u0026quot;), read_csv(\u0026quot;../Data/WonderData/YearCountyCauseRaceSuicidesAllYears.csv\u0026quot;), read_csv(\u0026quot;../Data/WonderData/YearCountyCauseRaceUnintentional1999-2007.csv\u0026quot;), read_csv(\u0026quot;../Data/WonderData/YearCountyCauseRaceUnintentional2008-2016.csv\u0026quot;), YearCountyCauseRaceHomicideAllYears) X2015_Gaz_counties_national \u0026lt;- read_csv(\u0026quot;../Data/WonderData/2015_Gaz_counties_national.csv\u0026quot;) PState \u0026lt;- read_csv(\u0026quot;../Data/Drug_Poisoning_Mortality_by_State__United_States.csv\u0026quot;) prescription\u0026lt;- read_csv(\u0026quot;../Data/Opioid analgesic prescriptions dispensed from US retail pharmacies, Q4 2009-Q2 2015.csv\u0026quot;) zip_codes \u0026lt;- read_csv(\u0026quot;../Data/zip_codes_states.csv\u0026quot;) stateabbr \u0026lt;- read.csv(\u0026quot;https://raw.githubusercontent.com/plotly/datasets/master/2011_us_ag_exports.csv\u0026quot;) load(\u0026quot;../Data/prescriber.Rda\u0026quot;) #cleans up the data that has age groups ageGroupsData \u0026lt;- YearCountyCauseAgeGroupsAllYears %\u0026gt;% mutate(percentTotalDeaths = parse_number(`% of Total Deaths`), crudeRateLower95Confint = parse_number(`Crude Rate Upper 95% Confidence Interval`), crudeRateUpper95Confint = parse_number(`Crude Rate Lower 95% Confidence Interval`), crudeRate = parse_number(`Crude Rate`), population = parse_number(Population)) %\u0026gt;% separate(County, c(\u0026quot;County\u0026quot;, \u0026quot;State\u0026quot;), sep = \u0026quot;,\u0026quot;) %\u0026gt;% left_join(X2015_Gaz_counties_national, by = c(\u0026quot;County\u0026quot; = \u0026quot;NAME\u0026quot;)) %\u0026gt;% rename(drugAlcoholInducedCause = `Drug/Alcohol Induced Cause`, ageGroups = `Ten-Year Age Groups Code`, latitude = INTPTLAT, longitude = INTPTLONG) %\u0026gt;% select(Year, County, State, drugAlcoholInducedCause, ageGroups, Deaths, population, crudeRate, crudeRateLower95Confint, crudeRateUpper95Confint, latitude, longitude) %\u0026gt;% mutate(State = str_trim(State), ageGroups = ifelse(ageGroups == \u0026quot;1\u0026quot;, \u0026quot;\u0026lt;1\u0026quot;, ageGroups), ageGroups = factor(ageGroups, levels = c(\u0026quot;\u0026lt;1\u0026quot;, \u0026quot;1-4\u0026quot;, \u0026quot;5-14\u0026quot;, \u0026quot;15-24\u0026quot;, \u0026quot;25-34\u0026quot;, \u0026quot;35-44\u0026quot;, \u0026quot;45-54\u0026quot;, \u0026quot;55-64\u0026quot;, \u0026quot;65-74\u0026quot;, \u0026quot;75-84\u0026quot;, \u0026quot;85+\u0026quot;))) #cleans up the data that has gender genderData \u0026lt;- YearCountyCauseGenderAllYears %\u0026gt;% mutate(percentTotalDeaths = parse_number(`% of Total Deaths`), crudeRateLower95Confint = parse_number(`Crude Rate Upper 95% Confidence Interval`), crudeRateUpper95Confint = parse_number(`Crude Rate Lower 95% Confidence Interval`), crudeRate = parse_number(`Crude Rate`), population = parse_number(Population), ageAdjRate = parse_number(`Age Adjusted Rate`), gender = as.factor(Gender))%\u0026gt;% separate(County, c(\u0026quot;County\u0026quot;, \u0026quot;State\u0026quot;), sep = \u0026quot;,\u0026quot;) %\u0026gt;% left_join(X2015_Gaz_counties_national, by = c(\u0026quot;County\u0026quot; = \u0026quot;NAME\u0026quot;)) %\u0026gt;% rename(drugAlcoholInducedCause = `Drug/Alcohol Induced Cause`, latitude = INTPTLAT, longitude = INTPTLONG) %\u0026gt;% select(Year, County, State, drugAlcoholInducedCause, Deaths, population, crudeRate, crudeRateLower95Confint, crudeRateUpper95Confint, ageAdjRate, gender, latitude, longitude) %\u0026gt;% mutate(State = str_trim(State)) #cleans up the data that has race raceData \u0026lt;- YearCountyCauseRaceAllYears %\u0026gt;% mutate(percentTotalDeaths = parse_number(`% of Total Deaths`), Year = parse_number(Year), crudeRateLower95Confint = parse_number(`Crude Rate Upper 95% Confidence Interval`), crudeRateUpper95Confint = parse_number(`Crude Rate Lower 95% Confidence Interval`), crudeRate = parse_number(`Crude Rate`), population = parse_number(Population), ageAdjRate = parse_number(`Age Adjusted Rate`), ageAdjRateLowerConfint = parse_number(`Age Adjusted Rate Lower 95% Confidence Interval`), ageAdjRateUpperConfint = parse_number(`Age Adjusted Rate Upper 95% Confidence Interval`), race = as.factor(Race), Deaths = parse_number(Deaths))%\u0026gt;% separate(County, c(\u0026quot;County\u0026quot;, \u0026quot;State\u0026quot;), sep = \u0026quot;,\u0026quot;) %\u0026gt;% left_join(X2015_Gaz_counties_national, by = c(\u0026quot;County\u0026quot; = \u0026quot;NAME\u0026quot;)) %\u0026gt;% rename(drugAlcoholInducedCause = `Drug/Alcohol Induced Cause`, latitude = INTPTLAT, longitude = INTPTLONG) %\u0026gt;% select(Year, County, State, drugAlcoholInducedCause, Deaths, population, crudeRate, crudeRateLower95Confint, crudeRateUpper95Confint, ageAdjRate, ageAdjRateLowerConfint, ageAdjRateUpperConfint,race, latitude, longitude) %\u0026gt;% mutate(State = str_trim(State)) #Select and rename relevant columns PState_wrangled \u0026lt;-PState %\u0026gt;% select(State,Year,Sex,`Age Group`,`Race and Hispanic Origin`,`Crude Death Rate` ,`Age-adjusted Rate`) %\u0026gt;% mutate(Age_Group=`Age Group`,Race=`Race and Hispanic Origin`,Crude_Death_Rate=`Crude Death Rate`,Age_Adjusted_Rate=`Age-adjusted Rate`) %\u0026gt;% select(-`Age Group`,-`Race and Hispanic Origin`,-`Crude Death Rate`,-`Age-adjusted Rate`)  #Parse and make column names more readable presc\u0026lt;-prescription %\u0026gt;% mutate(Yearly_totals_all_opioid_analgesics = as.integer(`Yearly totals (All Opioid Analgesics)`), Yearly_totals_HOTMF = as.integer(`Yearly totals (H+O+T+M+F)`), Yearly_totals_HO = as.integer(`Yearly totals (H+O)`, Yearly_totals_ER_LA_opioid_analgesics = as.integer(`Yearly totals (ER/LA Opioid Analgesics)`))) #top opioid prescribers prescribers\u0026lt;-prescriber %\u0026gt;% separate(`Opioid Prescribing Rate`, c(\u0026quot;Opioid_Prescribing_Rate\u0026quot;,\u0026quot;junk\u0026quot;), sep = \u0026quot;%\u0026quot;) %\u0026gt;% mutate(Opioid_Prescribing_Rate=as.integer(Opioid_Prescribing_Rate)) %\u0026gt;% select(-junk,-NPI) %\u0026gt;% group_by(`Specialty Description`) %\u0026gt;% summarize(count = n(),Opioid_Prescribing_Rate=sum(Opioid_Prescribing_Rate)/count) %\u0026gt;% drop_na() %\u0026gt;% arrange(desc(Opioid_Prescribing_Rate)) %\u0026gt;% head(4) #opioid prescription rates by zip code rates\u0026lt;-prescriber %\u0026gt;% separate(`Opioid Prescribing Rate`, c(\u0026quot;Opioid_Prescribing_Rate\u0026quot;,\u0026quot;junk\u0026quot;), sep = \u0026quot;%\u0026quot;) %\u0026gt;% mutate(Opioid_Prescribing_Rate=as.integer(Opioid_Prescribing_Rate), NPPES_zip_code=as.character(`NPPES Provider Zip Code`)) %\u0026gt;% select(-junk,-NPI,-`NPPES Provider Zip Code`) %\u0026gt;% group_by(NPPES_zip_code) %\u0026gt;% summarize(count=n(),Opioid_Prescribing_Rate=sum(Opioid_Prescribing_Rate)/count) %\u0026gt;% arrange(desc(Opioid_Prescribing_Rate))  #clean table containing state abbreviations stateabbr \u0026lt;- stateabbr %\u0026gt;% select(code,state) #Consolidate zipcode and prescribing rate data, death and drug info rates\u0026lt;-rates %\u0026gt;% left_join(zip_codes,by=c(\u0026quot;NPPES_zip_code\u0026quot; = \u0026quot;zip_code\u0026quot;)) %\u0026gt;% na.omit() %\u0026gt;% group_by(state) %\u0026gt;% summarize(count=n(), Opioid_Prescribing_Rate=sum(Opioid_Prescribing_Rate)/count) %\u0026gt;% arrange(desc(Opioid_Prescribing_Rate)) %\u0026gt;% head(4) #Wrangle state death records to display on map PS\u0026lt;-PState_wrangled %\u0026gt;% filter(Year==2014) %\u0026gt;% group_by(State) %\u0026gt;% summarize(count=n(),Crude_Death_Rate=sum(Crude_Death_Rate)/count, Age_Adjusted_Rate=sum(Age_Adjusted_Rate)/count) %\u0026gt;% filter(State!=\u0026quot;United States\u0026quot;) %\u0026gt;% left_join(stateabbr,by=c(\u0026quot;State\u0026quot; = \u0026quot;state\u0026quot;)) %\u0026gt;% select(State,Crude_Death_Rate,code,Age_Adjusted_Rate) %\u0026gt;% mutate(code=replace(code, State==\u0026quot;California\u0026quot;, \u0026quot;CA\u0026quot;),Crude_Death_Rate=round(Crude_Death_Rate, 1), Age_Adjusted_Rate=round(Age_Adjusted_Rate, 1))%\u0026gt;% left_join(rates,by=c(\u0026quot;code\u0026quot; = \u0026quot;state\u0026quot;))%\u0026gt;% mutate(Opioid_Prescribing_Rate=round(Opioid_Prescribing_Rate,1)) ageGroupsData %\u0026gt;% filter(ageGroups != \u0026quot;NS\u0026quot;, Year == 2016, drugAlcoholInducedCause == \u0026quot;Drug poisonings (overdose) Unintentional (X40-X44)\u0026quot;) %\u0026gt;% group_by(Year, ageGroups) %\u0026gt;% summarize(numObs = n(), TotalDeaths = sum(Deaths)) %\u0026gt;% plot_ly(x = ~ageGroups, y = ~TotalDeaths, type = \u0026quot;bar\u0026quot;, name = \u0026quot;bargraph\u0026quot;) raceData %\u0026gt;% filter(Year == 2016, drugAlcoholInducedCause == \u0026quot;Drug poisonings (overdose) Unintentional (X40-X44)\u0026quot;) %\u0026gt;% group_by(Year, race) %\u0026gt;% summarize(numObs = n(), TotalDeaths = sum(Deaths)) %\u0026gt;% plot_ly(x = ~race, y = ~TotalDeaths, type = \u0026quot;bar\u0026quot;, color = ~race)  genderData %\u0026gt;% filter(Year == 2016, drugAlcoholInducedCause == \u0026quot;Drug poisonings (overdose) Unintentional (X40-X44)\u0026quot;) %\u0026gt;% group_by(Year, gender) %\u0026gt;% summarize(numObs = n(), TotalDeaths = sum(Deaths)) %\u0026gt;% plot_ly(x = ~gender, y = ~TotalDeaths, type = \u0026quot;bar\u0026quot;, color = ~gender)  ageGroupsData %\u0026gt;% group_by(ageGroups, Year) %\u0026gt;% filter(drugAlcoholInducedCause == \u0026quot;Drug poisonings (overdose) Unintentional (X40-X44)\u0026quot;| drugAlcoholInducedCause == \u0026quot;All other drug-induced causes\u0026quot;| drugAlcoholInducedCause == \u0026quot;Drug poisonings (overdose) Suicide (X60-X64)\u0026quot;| drugAlcoholInducedCause == \u0026quot;Drug poisonings (overdose) Undetermined (Y10-Y14)\u0026quot;) %\u0026gt;% summarize(numObs = n(), TotalDeaths = sum(Deaths)) %\u0026gt;% plot_ly(x = ~Year, y = ~TotalDeaths, type = 'scatter', color = ~ageGroups, mode = \u0026quot;lines+markers\u0026quot;)  raceData %\u0026gt;% group_by(race, Year) %\u0026gt;% filter(drugAlcoholInducedCause == \u0026quot;Drug poisonings (overdose) Unintentional (X40-X44)\u0026quot;| drugAlcoholInducedCause == \u0026quot;All other drug-induced causes\u0026quot;| drugAlcoholInducedCause == \u0026quot;Drug poisonings (overdose) Suicide (X60-X64)\u0026quot;| drugAlcoholInducedCause == \u0026quot;Drug poisonings (overdose) Undetermined (Y10-Y14)\u0026quot;) %\u0026gt;% summarize(numObs = n(), TotalDeaths = sum(Deaths)) %\u0026gt;% plot_ly(x = ~Year, y = ~TotalDeaths, type = 'scatter', color = ~race, mode = \u0026quot;lines+markers\u0026quot;)  genderData %\u0026gt;% group_by(gender, Year) %\u0026gt;% filter(drugAlcoholInducedCause == \u0026quot;Drug poisonings (overdose) Unintentional (X40-X44)\u0026quot;| drugAlcoholInducedCause == \u0026quot;All other drug-induced causes\u0026quot;| drugAlcoholInducedCause == \u0026quot;Drug poisonings (overdose) Suicide (X60-X64)\u0026quot;| drugAlcoholInducedCause == \u0026quot;Drug poisonings (overdose) Undetermined (Y10-Y14)\u0026quot;) %\u0026gt;% summarize(numObs = n(), TotalDeaths = sum(Deaths)) %\u0026gt;% plot_ly(x = ~Year, y = ~TotalDeaths, type = 'scatter', color = ~gender, mode = \u0026quot;lines+markers\u0026quot;)  myTemp \u0026lt;- ageGroupsData %\u0026gt;% group_by(County) %\u0026gt;% summarize(NumObs = n(), SumDeaths = sum(Deaths), avgLong = mean(longitude), avgLat = mean(latitude)) %\u0026gt;% arrange(desc(SumDeaths)) %\u0026gt;% head(25) leaflet(myTemp) %\u0026gt;% addTiles() %\u0026gt;% addCircles(lng = ~avgLong, lat = ~avgLat, radius = ~SumDeaths/100) %\u0026gt;% setView(lng = -91.39, lat = 38.42, zoom = 5) #Exploratory visual showing trends in single states PState_wrangled %\u0026gt;% filter(State==\u0026quot;Wyoming\u0026quot;) %\u0026gt;% ggplot(.,aes(x=Year,y=Crude_Death_Rate,color=Sex))+geom_line()+facet_wrap(~Race) # give state boundaries a white border l \u0026lt;- list(color = toRGB(\u0026quot;white\u0026quot;), width = 2) # specify some map projection/options for plotly map g \u0026lt;- list( scope = 'usa', projection = list(type = 'albers usa'), showlakes = TRUE, lakecolor = toRGB('white') ) #Customize tooltip for death rates and prescriptions map PS$hover \u0026lt;- with(PS, paste(State,'\u0026lt;br\u0026gt;',\u0026quot;Crude death rate:\u0026quot;, Crude_Death_Rate,'\u0026lt;br\u0026gt;',\u0026quot;Opioid Prescribing Rate\u0026quot;, Opioid_Prescribing_Rate,'\u0026lt;br\u0026gt;',\u0026quot;\u0026lt;br\u0026gt;\u0026quot;)) #Make death rates and prescriptions plot plot_geo(PS, locationmode = 'USA-states') %\u0026gt;% add_trace( z = ~Age_Adjusted_Rate, text = ~hover, locations = ~code, color = ~Age_Adjusted_Rate, colors = 'Purples' ) %\u0026gt;% colorbar(title = \u0026quot;Rate\u0026quot;) %\u0026gt;% layout( title = 'Average death rates breakdown ', geo = g ) #Shows crude death rates by state br_down\u0026lt;-PState_wrangled %\u0026gt;% filter(State==\u0026quot;United States\u0026quot;) %\u0026gt;% group_by(State)  #Countrywide breakdown by race br_down %\u0026gt;% filter(Sex==\u0026quot;Female\u0026quot;,Age_Group==\u0026quot;All Ages\u0026quot;) %\u0026gt;% ggplot(.,aes(x=Year,y=Crude_Death_Rate))+geom_line()+facet_wrap(~Race)  .emojify { font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols; font-size: 2rem; vertical-align: middle; } @media screen and (max-width:650px) { .nowrap { display: block; margin: 25px 0; } } ","permalink":"https://adityatelange.github.io/hugo-PaperMod/posts/opioidcrisis/","summary":"\u003cp\u003eAn online tool which sheds more light on the numbers behind the opioid crisis\u003c/p\u003e","title":"Opioid Crisis Dashboard"}]